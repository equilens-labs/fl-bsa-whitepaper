# Whitepaper Intake Bundle — Consumption Instructions

**Audience:** Whitepaper workstream (data/analysis/docs)
**Status:** Ready to use (bundle generated by `make gate-wp`)

---

## Overview

The producer repo (`fl-bsa`) exports a compact bundle via `make gate-wp` with all per-pipeline intake artifacts:

**Required:**
- `selection_rates.csv`
- `metrics_uncertainty.json` (v4 SoT for fairness tables/plots)
- `provenance/manifest.json`
- `config/sap.yaml`
- `certificates/synthetic_quality_certificate.json` (SQ advisory text in v4 PDF)

**Optional (legacy/annex):**
- `metrics_long.csv` (legacy/back-compat; may include bootstrap methods)
- `group_confusion.csv` (label-based diagnostics; presence depends on scenario/config)

**Bundle locations:**
- Local (after a run): `artifacts/WhitePaper_Reviewer_Pack_v4.zip` or `output/<pipeline_id>/intake/`
- CI artifact: downloadable from fl-bsa CI runs

---

## Generate Bundle (Producer: fl-bsa)

```bash
cd /path/to/fl-bsa

# Full evidence generation (starts services, runs pipeline, validates, packages)
make gate-wp

# Outputs: artifacts/WhitePaper_Reviewer_Pack_v4.zip
```

The gate-wp target:
1. Starts FL-BSA services (API, Worker, Redis)
2. Generates a seeded synthetic dataset
3. Submits to pipeline for full bias analysis
4. Validates intake artifacts with strict schema checking
5. Repairs provenance with real hashes/digests
6. Packages everything into `WhitePaper_Reviewer_Pack_v4.zip`

---

## Consume Bundle (This Repo)

### Manual Copy
```bash
# Unzip bundle
unzip artifacts/WhitePaper_Reviewer_Pack_v4.zip -d /tmp/bundle

# Copy intake files
cp /tmp/bundle/intake/*.csv intake/
cp /tmp/bundle/intake/*.json intake/
cp /tmp/bundle/provenance/manifest.json intake/manifest.json
mkdir -p intake/certificates && cp /tmp/bundle/certificates/*.json intake/certificates/
cp /tmp/bundle/config/sap.yaml config/sap.yaml
cp /tmp/bundle/config/fairness_config.yaml config/fairness_config.yaml

# Generate LaTeX macros and build PDF
make pdf
```

### CI Pull
The `.github/workflows/pull-wp-intake.yml` workflow can be configured to:
- Trigger on producer CI completion or on demand
- Download the reviewer bundle artifact from the producer repo
- Copy files to `intake/` and regenerate PDF

---

## Bundle Layout

### Produced by gate-wp:
```
WhitePaper_Reviewer_Pack_v4.zip
├── intake/
│   ├── selection_rates.csv
│   ├── metrics_long.csv
│   ├── metrics_uncertainty.json (v4 SoT)
│   ├── group_confusion.csv (optional; EO only)
│   ├── regulatory_matrix.csv
│   └── calibration_bins.csv
├── provenance/
│   └── manifest.json
├── config/
│   └── sap.yaml
│   └── fairness_config.yaml
├── certificates/
│   └── *.json
└── privacy/
    └── tests/
        ├── privacy_evidence_membership.json
        ├── privacy_evidence_attribute.json
        └── dp_accounting.json
```

---

## CSV Schemas

### selection_rates.csv
- **Header:** `run_id,split,model_id,attribute,group,selected,n`
- **Meaning:** Per `(split, attribute, group)`; `selected` is count of `loan_approved=1`; `n` is group size (0 ≤ selected ≤ n).

### metrics_uncertainty.json (v4 SoT)
- **Format:** JSON (schema version `fairness_uncertainty.v1`)
- **Meaning:** Deterministic fairness uncertainty surface (AIR + approval-rate gap), including race multi-class pairwise vs reference.

### metrics_long.csv
- **Header:** `run_id,split,model_id,metric,group,value,lower_ci,upper_ci,n,method`
- **Rows:**
  - `selection_rate` for each `attribute:group` (95% CI, `method=wilson`)
  - Other legacy/annex metrics (may include bootstrap methods); **not SoT for the v4 PDF**

### group_confusion.csv (if present)
- **Header:** `run_id,split,model_id,attribute,group,TP,FP,TN,FN`
- **Computed with:** `y_true=ground_truth`, `y_pred=loan_approved`

---

## Provenance Handling

**File:** `provenance/manifest.json`

**Keys:**
- `schema_version` — manifest schema version
- `run_id` — unique pipeline run identifier
- `created` — ISO timestamp
- `dataset_hash` — `sha256:<hex>` of input dataset
- `commit_sha` — git commit of producer code
- `container_digests` — API and worker image digests
- `seeds` — RNG seeds for reproducibility
- `config_hash` — hash of configuration
- `capabilities` — flags like `eo_enabled`, `ece_enabled`

**Usage:**
- Cite `commit_sha` in the paper as the build reference
- Use `dataset_hash` for reproducibility
- If EO is required, check `capabilities.eo_enabled`

---

## EO (Equalized Odds) Notes

- EO is optional and only present when `ground_truth` is available
- For EO-required analyses, verify `group_confusion.csv` exists
- gate-wp includes EO when `WP_ENABLE_EO=1` (default in full runs)

---

## Integrity Checks

Validate shapes/values before consumption:
- Headers exactly match for all CSVs
- `selection_rate`: 0 ≤ value ≤ 1 and `lower_ci ≤ value ≤ upper_ci`
- `selection_rate` equals `selected/n` within tolerance
- `air` recomputed from selection rates matches within tolerance

The producer repo includes a validator:
```bash
# In fl-bsa (root contains one or more pipeline directories)
python tools/ci/validate_wp_intake.py --root output
```

---

## Common Pitfalls

| Issue | Cause | Fix |
|-------|-------|-----|
| No bundle | gate-wp not run | Run `make gate-wp` in fl-bsa |
| Placeholder provenance | Local dev mode | Set `CONTAINER_REF` env or run in CI |
| Missing EO | No ground_truth in dataset | Enable with `WP_ENABLE_EO=1` |
| Stale metrics | Old bundle | Re-run gate-wp for fresh evidence |

---

## Programmatic Loader (Optional)

```python
from pathlib import Path
import json
import pandas as pd

def load_intake(intake_dir: str = "intake"):
    root = Path(intake_dir)
    return {
        "selection_rates": pd.read_csv(root / "selection_rates.csv") if (root / "selection_rates.csv").exists() else pd.DataFrame(),
        "metrics_uncertainty": json.loads((root / "metrics_uncertainty.json").read_text()) if (root / "metrics_uncertainty.json").exists() else {},
        "metrics_long": pd.read_csv(root / "metrics_long.csv") if (root / "metrics_long.csv").exists() else pd.DataFrame(),
        "group_confusion": pd.read_csv(root / "group_confusion.csv") if (root / "group_confusion.csv").exists() else pd.DataFrame(),
        "manifest": json.loads((root / "manifest.json").read_text()) if (root / "manifest.json").exists() else {},
    }
```
